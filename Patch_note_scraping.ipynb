{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a15598c-aeda-4a99-b35c-f4e2b1a6e323",
   "metadata": {},
   "source": [
    "# No Man's Sky (Hello Games) Patch Note Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c70a3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.diagnose import diagnose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09593167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_page_urls(\n",
    "    url=\"https://www.nomanssky.com/\",\n",
    "    releaseLogPage=\"/release-log/\",\n",
    "    patchNoteSection=(\"a\",\"link link--inherit\")\n",
    "):\n",
    "    \n",
    "    page = requests.get(url+releaseLogPage)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    ## Grab the list of individual patch release pages \n",
    "    if len(patchNoteSection)==1:\n",
    "        ## UNTESTED\n",
    "        htmlTarget = patchNoteSection[0]\n",
    "        patchPages = soup.find_all(htmlTarget, href=True)\n",
    "    elif len(patchNoteSection)==2:\n",
    "        htmlTarget = patchNoteSection[0]\n",
    "        htmlClass = patchNoteSection[1]\n",
    "        patchPages = soup.find_all(htmlTarget, class_=htmlClass) \n",
    "    else:\n",
    "        raise ValueError(\"Invalid patchNoteSection format.\")\n",
    "     \n",
    "    links = [patch.get(\"href\") for patch in patchPages]\n",
    "\n",
    "    ## Clean links of any base URL ('url' variable) content. \n",
    "    ## This also helps address bugs introduced wherein some pages links are given in\n",
    "    ##   `www.nommanssky.com` format (equivalent to http://www.nommanssky.com) rather than\n",
    "    ## the expected \n",
    "    ##   `https://www.nomanssky.com/` format.\n",
    "    splitLinks = [link.split(\".com\")[-1] if len(link.split(\".com\"))>1 else link for link in links]\n",
    "\n",
    "    ## Clean whitespace\n",
    "    finalLinks = [link.strip() for link in splitLinks]\n",
    "    \n",
    "    return finalLinks\n",
    "\n",
    "def _class_not_patch_note_button(class_):\n",
    "    if not class_:\n",
    "        return True  # Exclude elements with no class\n",
    "\n",
    "    ## check to ensure class_ var hasn't been passed a list\n",
    "    if isinstance(class_, list):\n",
    "        ## in which case we need to examine the list rather than an exact match\n",
    "        if (\"btn\" in class_ and \"btn--primary\" in class_): \n",
    "            return False  # Exclude if both are present\n",
    "    \n",
    "    return class_ != \"btn btn--primary\"  # Default behaviour: return True when not patch note button, False when it is an exact match with button definition\n",
    "\n",
    "def get_patch_notes_from_page(\n",
    "    patchPageUrl,\n",
    "    baseUrl=\"https://www.nomanssky.com/\",\n",
    "    identifyPatchNotesString=\"Patch Notes\",\n",
    "    identifyBugFixesString=\"Bug Fixes\"\n",
    "):\n",
    "    output = {\n",
    "        \"Title\": None,\n",
    "        \"Date\": None,\n",
    "        \"Notes\": None,\n",
    "        \"Patch update\": False,\n",
    "        \"Bug fix update\": False,\n",
    "        \"Scrape error\": False,\n",
    "        \"Patch page url\": copy.deepcopy(baseUrl)+copy.deepcopy(patchPageUrl),\n",
    "    }\n",
    "\n",
    "    page = requests.get(baseUrl+patchPageUrl)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        \n",
    "    ## Try default page title assignment\n",
    "    try:\n",
    "        releaseTitleNote = soup.find(class_=\"text--heading-centered margin--bottom-default\")\n",
    "        releaseTitleString = str(list(releaseTitleNote.stripped_strings)[0])\n",
    "        output[\"Title\"] = releaseTitleString\n",
    "    except:\n",
    "        output[\"Title\"] = None \n",
    "\n",
    "    rawUrlNote = patchPageUrl.split(\"/\")\n",
    "    urlNote = [xx for xx in rawUrlNote if xx] # Clean out any empty strings from 'str.split(\"/\")'\n",
    "    \n",
    "    ## Backup method to find release title\n",
    "    if output[\"Title\"] == None:\n",
    "        ## Title patch page entry by default naming convention of NMS URLs    \n",
    "        output[\"Title\"] = urlNote[-1]\n",
    "    \n",
    "\n",
    "    ## Attempt to assign date\n",
    "    try:\n",
    "        ## Frequently the date occurs in a html span objecth class_==\"date\"\n",
    "        ## This tends to be given in format of, for example: \"August 18, 2016.\"\n",
    "        dateNote = soup.find(class_=\"date\")\n",
    "        dateString = str(list(dateNote.stripped_strings)[0])\n",
    "        ## Note the dropped final full-stop. `stripped_strings` line above seems to remove this...\n",
    "        ## We also want to remove the comma, as there are edge-cases where the comma is absent\n",
    "        output[\"Date\"] = datetime.strptime(dateString.replace(\",\",\"\"),\"%B %d %Y\")\n",
    "    except:\n",
    "        ## Okay, no date in this format\n",
    "        output[\"Date\"] = None\n",
    "\n",
    "    ## Attempt backup. so, if we found no date, try the url format instead\n",
    "    if output[\"Date\"] == None:\n",
    "        try:\n",
    "            dateNote = urlNote[-2]\n",
    "            ## No day of month given in the url format, so will default to 1st of the month for all\n",
    "            dateNote = dateNote + [\"01\"]\n",
    "            dateString = \" \".join(dateNote) \n",
    "            dateString = dateString.strip()\n",
    "            output[\"Date\"] = datetime.strptime(dateString,\"%Y %m %d\")\n",
    "        except:\n",
    "            ## Okay, no date in this format either. For now, set this Date value to None.\n",
    "            output[\"Date\"] = None\n",
    "\n",
    "        \n",
    "    ## Retrieve all elements that contain the patch notes string\n",
    "    allNotes = soup.find_all(string=re.compile(identifyPatchNotesString, flags=re.IGNORECASE))\n",
    "    \n",
    "    ## Filter out elements with \"btn btn--primary\"\n",
    "    noteSection = [tag for tag in allNotes if _class_not_patch_note_button(tag.parent.get(\"class\"))]  \n",
    "\n",
    "    ## If no patch notes found in this format, check old patch release page format\n",
    "    ## Where patch note string is just in `Patch [number]` format\n",
    "    ## Frequently these seem to actually be Bug Fixes, and should be flagged accordingly...\n",
    "    if not noteSection:\n",
    "        output[\"Bug fix update\"] = True\n",
    "        oldIdentifyPatchNotesString = identifyPatchNotesString.split(\" \")[0]\n",
    "        ## Retrieve all elements that contain the patch notes string\n",
    "        allNotes = soup.find_all(string=re.compile(oldIdentifyPatchNotesString, flags=re.IGNORECASE))\n",
    "    \n",
    "        ## Filter out elements with \"btn btn--primary\"\n",
    "        noteSection = [tag for tag in allNotes if _class_not_patch_note_button(tag.parent.get(\"class\"))]  \n",
    "        \n",
    "    \n",
    "    ## If no Patch Notes info has been found, try scraping for bug fix notes instead\n",
    "    if not noteSection:\n",
    "        output[\"Bug fix update\"] = True\n",
    "        ## Retrieve all elements that contain the bug fix string\n",
    "        allNotes = soup.find_all(string=re.compile(identifyBugFixesString, flags=re.IGNORECASE))\n",
    "\n",
    "        ## Filter out elements with \"btn btn--primary\"\n",
    "        noteSection = [tag for tag in allNotes if _class_not_patch_note_button(tag.parent.get(\"class\"))]  \n",
    "\n",
    "\n",
    "    if output[\"Bug fix update\"] == False:\n",
    "        output[\"Patch update\"] = True\n",
    "    \n",
    "    ## Now that we've located the relevant section of the page, let us go\n",
    "    ##  back up the beautiful soup html tree, to identify the notes that follow\n",
    "    notes = []\n",
    "    for nn in noteSection:\n",
    "        ## We need to climb several levels. At least one for the string itself, one for the placeholder of that string on page\n",
    "        ##  and one more for that page section. However, there may be more, hence the while loop.\n",
    "        ## Finally, we should find the section containing the patch/bug fix notes\n",
    "\n",
    "        parentLevel = 0\n",
    "        pageSection = nn.parent\n",
    "        search = pageSection.find_all(\"li\")\n",
    "\n",
    "        while ((parentLevel <=5)&(len(search)<=3)):\n",
    "            newSection = pageSection.parent\n",
    "            search = newSection.find_all(\"li\")\n",
    "            pageSection = newSection\n",
    "            parentLevel+=1\n",
    "\n",
    "        for ee in pageSection.stripped_strings:\n",
    "            notes.append(repr(ee))\n",
    "\n",
    "    if not notes:\n",
    "        output[\"Scrape error\"] = True\n",
    "        \n",
    "    output[\"Notes\"] = \"\\n\".join(notes)\n",
    "    return output\n",
    "\n",
    "def get_patch_notes(\n",
    "    url=\"https://www.nomanssky.com/\",\n",
    "    releaseLogPage=\"/release-log/\",\n",
    "    patchNoteSection=(\"a\",\"link link--inherit\"),\n",
    "    identifyPatchNotesString=\"Patch Notes\",\n",
    "    identifyBugFixesString=\"Bug Fixes\"):\n",
    "\n",
    "    out = {}\n",
    "    print(f\"Finding individual patch release pages from {url+releaseLogPage} ...\")\n",
    "    patchPages = get_patch_page_urls(\n",
    "        url=url,\n",
    "        releaseLogPage=releaseLogPage,\n",
    "        patchNoteSection=patchNoteSection\n",
    "    )\n",
    "\n",
    "    nPages = len(patchPages)\n",
    "    print(f\"Found {nPages} individual patch release pages!\")\n",
    "    print(f\"Scraping patch notes...\")\n",
    "    # print(f\"DEBUG mode active!! Pages gathered limited!!!!\")\n",
    "    for ii,page in tqdm(enumerate(patchPages),total=nPages):\n",
    "        ## Reduce request frequency in less 'robotic' fashion to prevent sites blocking access\n",
    "        time.sleep(random.uniform(0.0,3.0))\n",
    "        notes = get_patch_notes_from_page(\n",
    "            patchPageUrl=page,\n",
    "            baseUrl=url,\n",
    "            identifyPatchNotesString=identifyPatchNotesString,\n",
    "            identifyBugFixesString=identifyBugFixesString\n",
    "        )\n",
    "        out[ii] = copy.deepcopy(notes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f328d34-187f-41be-834c-2cc87f9878dd",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c75ed-3e45-41b2-a64d-a88a27fe91be",
   "metadata": {},
   "source": [
    "### Part 1: Scrape NMS webpage data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1328331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding individual patch release pages from https://www.nomanssky.com//release-log/ ...\n",
      "Found 239 individual patch release pages!\n",
      "Scraping patch notes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 239/239 [08:55<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "nmsPatchNotes = get_patch_notes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105434e9-7231-45c1-8a75-3efb91aa6926",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa464ba5-1918-45fe-b083-1246ed90e8ac",
   "metadata": {},
   "source": [
    "### Convert scraped data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11227eb-f62f-43c6-80eb-a44963081f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(nmsPatchNotes, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bd3513b-9edf-42f9-859e-29111d4b5a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Title       Date  \\\n",
      "0        Worlds Part II - 5.58 2025-03-04   \n",
      "1        Worlds Part II - 5.57 2025-02-14   \n",
      "2        Worlds Part II - 5.56 2025-02-13   \n",
      "3  Expedition Seventeen: Titan 2025-02-12   \n",
      "4        Worlds Part II - 5.55 2025-02-12   \n",
      "\n",
      "                                               Notes  Patch update  \\\n",
      "0  'Worlds Part II - 5.58'\\n'March 04, 2025'\\n'.'...         False   \n",
      "1  'Worlds Part II - 5.57'\\n'February 14, 2025'\\n...         False   \n",
      "2  'Worlds Part II - 5.56'\\n'February 13, 2025'\\n...         False   \n",
      "3  'Expedition Seventeen: Titan'\\n'February 12, 2...          True   \n",
      "4  'Worlds Part II - 5.55'\\n'February 12, 2025'\\n...         False   \n",
      "\n",
      "   Bug fix update  Scrape error  \\\n",
      "0            True         False   \n",
      "1            True         False   \n",
      "2            True         False   \n",
      "3           False         False   \n",
      "4            True         False   \n",
      "\n",
      "                                      Patch page url  \n",
      "0  https://www.nomanssky.com//2025/03/worlds-part...  \n",
      "1  https://www.nomanssky.com//2025/02/worlds-part...  \n",
      "2  https://www.nomanssky.com//2025/02/worlds-part...  \n",
      "3  https://www.nomanssky.com//2025/02/expedition-...  \n",
      "4  https://www.nomanssky.com//2025/02/worlds-part...   \n",
      "\n",
      " ====================\n",
      "                                Date\n",
      "count                            210\n",
      "mean   2021-06-22 16:27:25.714285824\n",
      "min              2016-08-18 00:00:00\n",
      "25%              2019-08-24 06:00:00\n",
      "50%              2021-09-02 12:00:00\n",
      "75%              2023-06-18 12:00:00\n",
      "max              2025-03-04 00:00:00 \n",
      "\n",
      " ====================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 239 entries, 0 to 238\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Title           239 non-null    object        \n",
      " 1   Date            210 non-null    datetime64[ns]\n",
      " 2   Notes           239 non-null    object        \n",
      " 3   Patch update    239 non-null    bool          \n",
      " 4   Bug fix update  239 non-null    bool          \n",
      " 5   Scrape error    239 non-null    bool          \n",
      " 6   Patch page url  239 non-null    object        \n",
      "dtypes: bool(3), datetime64[ns](1), object(3)\n",
      "memory usage: 10.0+ KB\n",
      "None \n",
      "\n",
      " ====================\n"
     ]
    }
   ],
   "source": [
    "print(df.head(),\"\\n\"*2,\"=\"*20)\n",
    "print(df.describe(),\"\\n\"*2,\"=\"*20)\n",
    "print(df.info(),\"\\n\"*2,\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f053ed4-bd85-41ab-8f0c-572ac5b56812",
   "metadata": {},
   "source": [
    "Convert dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6daaff-c699-424f-abf5-c22b9d2cf4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'Title':str,'Date':\"datetime64[ns]\",'Bug fix update':\"boolean\",'Patch update':\"boolean\",'Scrape error':\"boolean\",'Notes':str,'Patch page url':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c101ad8a-4b7a-4236-aaa3-daa242c4ec91",
   "metadata": {},
   "source": [
    "Sort and reverse row indices --- this (hopefully) future-proofs the dataset if/when new patches are released by having the latest patch be the page with the highest index value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e311cb-2715-4022-b19b-d646d2786e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[::-1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733f186-ef14-4ab5-8eec-618c2ba796f3",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36b26c5e-0589-4ca8-8f7a-efd15cc3aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Title       Date  \\\n",
      "0  PC Patch 1.04 2016-08-18   \n",
      "1  PC Patch 1.05 2016-08-19   \n",
      "2  PC Patch 1.06 2016-08-20   \n",
      "3  PC Patch 1.07 2016-09-02   \n",
      "4  PC Patch 1.08 2016-09-04   \n",
      "\n",
      "                                               Notes  Patch update  \\\n",
      "0  \"PC Patch 1.04 - No Man's Sky\"\\n'Menu'\\n'Lates...         False   \n",
      "1  \"PC Patch 1.05 - No Man's Sky\"\\n'Menu'\\n'Lates...         False   \n",
      "2  \"PC Patch 1.06 - No Man's Sky\"\\n'Menu'\\n'Lates...         False   \n",
      "3  \"PC Patch 1.07 - No Man's Sky\"\\n'Menu'\\n'Lates...         False   \n",
      "4  \"PC Patch 1.08 - No Man's Sky\"\\n'Menu'\\n'Lates...         False   \n",
      "\n",
      "   Bug fix update  Scrape error                             Patch page url  \n",
      "0            True         False  https://www.nomanssky.com//pc-patch-1-04/  \n",
      "1            True         False  https://www.nomanssky.com//pc-patch-1-05/  \n",
      "2            True         False  https://www.nomanssky.com//pc-patch-1-06/  \n",
      "3            True         False  https://www.nomanssky.com//pc-patch-1-07/  \n",
      "4            True         False  https://www.nomanssky.com//pc-patch-1-08/   \n",
      "\n",
      " ====================\n",
      "                                Date\n",
      "count                            210\n",
      "mean   2021-06-22 16:27:25.714285824\n",
      "min              2016-08-18 00:00:00\n",
      "25%              2019-08-24 06:00:00\n",
      "50%              2021-09-02 12:00:00\n",
      "75%              2023-06-18 12:00:00\n",
      "max              2025-03-04 00:00:00 \n",
      "\n",
      " ====================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 239 entries, 0 to 238\n",
      "Data columns (total 7 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Title           239 non-null    object        \n",
      " 1   Date            210 non-null    datetime64[ns]\n",
      " 2   Notes           239 non-null    object        \n",
      " 3   Patch update    239 non-null    boolean       \n",
      " 4   Bug fix update  239 non-null    boolean       \n",
      " 5   Scrape error    239 non-null    boolean       \n",
      " 6   Patch page url  239 non-null    object        \n",
      "dtypes: boolean(3), datetime64[ns](1), object(3)\n",
      "memory usage: 9.0+ KB\n",
      "None \n",
      "\n",
      " ====================\n"
     ]
    }
   ],
   "source": [
    "print(df.head(),\"\\n\"*2,\"=\"*20)\n",
    "print(df.describe(),\"\\n\"*2,\"=\"*20)\n",
    "print(df.info(),\"\\n\"*2,\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1b3a5-6066-496f-9c2a-e61bf17e90ed",
   "metadata": {},
   "source": [
    "### Part 2: Scrape wikipedia.org NMS page to get named releases' date of release data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a6b0401c-63e8-4d99-80c0-cea89ef2cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipediaURL =\"https://en.wikipedia.org/wiki/No_Man%27s_Sky\"\n",
    "page = requests.get(wikipediaURL)\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c2233ee-536c-4aa3-b267-dd5363ea41b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for caption in soup.find_all(\"caption\"):\n",
    "    captionsList = list(caption.strings)\n",
    "    tokenisedCaptions = [capt.lower().split(\" \") for capt in captionsList]\n",
    "    for capt in tokenisedCaptions:\n",
    "        if (\"list\" in capt)&(\"updates\" in capt):\n",
    "            table = caption.find_parent(\"table\", {\"class\": \"wikitable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0149189c-f018-43d3-85db-88ce6338a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiData = {}\n",
    "for ii, entry in enumerate(table.find_all(\"tr\")):\n",
    "    output = {\n",
    "        \"Title\": None,\n",
    "        \"Date\": None,\n",
    "        \"Notes\": None,\n",
    "        \"Ref\": None\n",
    "        }\n",
    "    content = [\" \".join(list(val.strings)) for val in entry.contents if val != \"\\n\"]\n",
    "    output[\"Date\"], output[\"Title\"], output[\"Notes\"], output[\"Ref\"] = content\n",
    "\n",
    "    ## If the entry is a valid row, it will have a date associated with it. The table headers row, however, will not. So we will skip that first row with this try, except block.\n",
    "    try:\n",
    "        dateString = output[\"Date\"].strip()\n",
    "        output[\"Date\"] = datetime.strptime(dateString,\"%B %Y\")\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    wikiData[ii] = output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c36bcad8-9d00-4dc9-8228-90afae70f4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Foundation\\n</td>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>User-made bases built from modular components,...</td>\n",
       "      <td>[ 38 ] \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pathfinder\\n</td>\n",
       "      <td>2017-03-01</td>\n",
       "      <td>Ability to share bases with other players New ...</td>\n",
       "      <td>[ 39 ] [ 40 ] [ 41 ] \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Atlas Rises\\n</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>Addition of new narrative branch to main game'...</td>\n",
       "      <td>[ 43 ] [ 42 ] \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Next\\n</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>Added support for  Xbox One  and  WeChat Full ...</td>\n",
       "      <td>[ 44 ] [ 45 ] [ 46 ] [ 47 ] [ 48 ] [ 49 ] [ 50...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Abyss\\n</td>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>Expanded aquatic biomes features include new c...</td>\n",
       "      <td>[ 52 ] \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Title       Date  \\\n",
       "1   Foundation\\n 2016-11-01   \n",
       "2   Pathfinder\\n 2017-03-01   \n",
       "3  Atlas Rises\\n 2017-08-01   \n",
       "4         Next\\n 2018-07-01   \n",
       "5    The Abyss\\n 2018-10-01   \n",
       "\n",
       "                                               Notes  \\\n",
       "1  User-made bases built from modular components,...   \n",
       "2  Ability to share bases with other players New ...   \n",
       "3  Addition of new narrative branch to main game'...   \n",
       "4  Added support for  Xbox One  and  WeChat Full ...   \n",
       "5  Expanded aquatic biomes features include new c...   \n",
       "\n",
       "                                                 Ref  \n",
       "1                                          [ 38 ] \\n  \n",
       "2                            [ 39 ] [ 40 ] [ 41 ] \\n  \n",
       "3                                   [ 43 ] [ 42 ] \\n  \n",
       "4  [ 44 ] [ 45 ] [ 46 ] [ 47 ] [ 48 ] [ 49 ] [ 50...  \n",
       "5                                          [ 52 ] \\n  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiDF =  pd.DataFrame.from_dict(wikiData, orient=\"index\")\n",
    "wikiDF = wikiDF.astype({'Title':str,'Date':\"datetime64[ns]\",'Notes':str,'Ref':str})\n",
    "wikiDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b067e-9713-4e00-af98-fee62da959b6",
   "metadata": {},
   "source": [
    "### Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4327e47e-feec-4446-a2ed-214404804fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"./Data/Raw/\"\n",
    "saveFile = \"NMS_patch_notes.xlsx\"\n",
    "\n",
    "tmp = \"\"\n",
    "for savePathChunk in savePath.split(\"/\")[:-1]:\n",
    "    tmp += savePathChunk + \"/\"\n",
    "    try:\n",
    "        os.mkdir(tmp)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "with pd.ExcelWriter(path=savePath+saveFile,mode=\"w\") as writer:\n",
    "    df.to_excel(writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e59add76-dc9f-4218-a875-be4723041643",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"./Data/Raw/\"\n",
    "saveFile = \"NMS_wikipedia_major_releases_notes.xlsx\"\n",
    "\n",
    "tmp = \"\"\n",
    "for savePathChunk in savePath.split(\"/\")[:-1]:\n",
    "    tmp += savePathChunk + \"/\"\n",
    "    try:\n",
    "        os.mkdir(tmp)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "with pd.ExcelWriter(path=savePath+saveFile,mode=\"w\") as writer:\n",
    "    wikiDF.to_excel(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a296113-09d9-4449-9997-f46f672f80e1",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1a175-cf86-46c5-ac9f-a50d5862a193",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
